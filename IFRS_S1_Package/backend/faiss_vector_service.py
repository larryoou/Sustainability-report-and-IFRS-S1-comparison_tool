#!/usr/bin/env python3
"""
FAISSå‘é‡æª¢ç´¢æœå‹™ - å¹³è¡¡çµ„åˆå„ªåŒ–æ–¹æ¡ˆ
å¯¦ç¾IFRS S1æ¢æ–‡å‘é‡é è¨ˆç®—å’Œé«˜é€Ÿæª¢ç´¢
ç›®æ¨™ï¼š5-8åˆ†é˜å…§å®Œæˆæ°¸çºŒå ±å‘Šæ›¸åˆ†æ
"""

import os
import json
import pickle
import time
import numpy as np
import faiss
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import logging
from concurrent.futures import ThreadPoolExecutor
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# IFRS S1 æ ¸å¿ƒè©å½™æ˜ å°„å­—å…¸
VOCABULARY_MAPPING = {
    # æ²»ç†ç›¸é—œ
    "æ²»ç†": ["è‘£äº‹æœƒ", "æ²»ç†å§”å“¡æœƒ", "ç®¡ç†å±¤", "æ±ºç­–æ©Ÿé—œ", "ç®¡ç†éšå±¤", "å…¬å¸æ²»ç†"],
    "æ²»ç†æ©Ÿæ§‹": ["è‘£äº‹æœƒ", "æ²»ç†å§”å“¡æœƒ", "ç®¡ç†å±¤", "æ±ºç­–æ©Ÿé—œ", "ç®¡ç†éšå±¤"],
    "ç›£æ¸¬": ["ç›£ç£", "ç›£æ§", "è¿½è¹¤", "æª¢è¦–", "è§€å¯Ÿ", "å¯©è¦–", "æŠŠé—œ"],
    "ç®¡ç†": ["ç®¡æ§", "è™•ç†", "åŸ·è¡Œ", "ç‡Ÿé‹", "ç¶“ç‡Ÿ", "æ“ä½œ", "é‹ä½œ"],
    "ç›£ç£": ["ç›£ç®¡", "ç£å°", "æŸ¥æ ¸", "ç¨½æ ¸", "oversight"],
    
    # æŠ«éœ²å ±å‘Šç›¸é—œ
    "æŠ«éœ²": ["æ­éœ²", "å ±å‘Š", "å…¬å¸ƒ", "ç™¼å¸ƒ", "èªªæ˜", "å‘ˆç¾", "å±•ç¤º", "é€éœ²", "è² è²¬"],
    "å ±å‘Š": ["å ±å‘Šæ›¸", "å¹´å ±", "æ°¸çºŒå ±å‘Š", "CSRå ±å‘Š", "ESGå ±å‘Š"],
    "æ­éœ²": ["å…¬é–‹", "é€æ˜åŒ–", "è³‡è¨Šå…¬é–‹", "è³‡æ–™æ­ç¤º"],
    "è³‡è¨Š": ["è³‡æ–™", "æ•¸æ“š", "ä¿¡æ¯", "å…§å®¹", "è¨Šæ¯"],
    
    # é¢¨éšªç®¡ç†ç›¸é—œ
    "é¢¨éšª": ["é¢¨éšªå› å­", "ä¸ç¢ºå®šæ€§", "æŒ‘æˆ°", "å¨è„…", "å±éšªå› ç´ "],
    "æ©Ÿé‡": ["æ©Ÿæœƒ", "å•†æ©Ÿ", "ç™¼å±•æ©Ÿæœƒ", "æˆé•·å‹•èƒ½", "æ½›åŠ›", "å¥‘æ©Ÿ"],
    "é¢¨éšªç®¡ç†": ["é¢¨æ§", "é¢¨éšªæ§åˆ¶", "é¢¨éšªé˜²ç¯„", "é¢¨éšªå› æ‡‰"],
    "è©•ä¼°": ["è©•é‡", "è©•é‘‘", "åˆ†æ", "æª¢è¨", "å¯©æŸ¥", "è¡¡é‡"],
    
    # æ°£å€™è®ŠåŒ–ç›¸é—œ
    "æ°£å€™ç›¸é—œ": ["æ°£å€™è®Šé·", "æ°£å€™è®ŠåŒ–", "æ°£å€™é¢¨éšª", "ä½ç¢³", "æ¸›ç¢³"],
    "æ°£å€™": ["æ°£å€™è®Šé·", "æ°£å€™è®ŠåŒ–", "ç’°å¢ƒ", "ç¢³", "æº«å®¤æ°£é«”"],
    "æº«å®¤æ°£é«”": ["GHG", "CO2", "ç¢³æ’æ”¾", "æ’æ”¾é‡", "ç¢³è¶³è·¡"],
    "æ¸›ç·©": ["æ¸›é‡", "é™ä½", "å‰Šæ¸›", "ç¸®æ¸›", "æŠ‘åˆ¶"],
    "èª¿é©": ["é©æ‡‰", "å› æ‡‰", "èª¿æ•´", "è½‰å‹", "æ‡‰å°"],
    
    # ç­–ç•¥ç›¸é—œ
    "ç­–ç•¥": ["æˆ°ç•¥", "æ–¹é‡", "æ”¿ç­–", "è¦åŠƒ", "è—åœ–", "è¨ˆç•«"],
    "å•†æ¥­æ¨¡å¼": ["ç‡Ÿé‹æ¨¡å¼", "ç¶“ç‡Ÿæ¨¡å¼", "å•†æ¥­ç­–ç•¥", "ç²åˆ©æ¨¡å¼"],
    "è¦åŠƒ": ["è¨ˆåŠƒ", "å¸ƒå±€", "å®‰æ’", "è¨­è¨ˆ", "ç±ŒåŠƒ"],
    "ç›®æ¨™": ["æŒ‡æ¨™", "KPI", "é‡Œç¨‹ç¢‘", "æ¨™çš„", "é¡˜æ™¯"],
    
    # æŒ‡æ¨™ç›®æ¨™ç›¸é—œ
    "æŒ‡æ¨™": ["æŒ‡æ•¸", "æ•¸æ“š", "é‡åŒ–æŒ‡æ¨™", "è¡¡é‡æ¨™æº–", "ç¸¾æ•ˆæŒ‡æ¨™"],
    "ç¸¾æ•ˆ": ["è¡¨ç¾", "æˆæœ", "æ¥­ç¸¾", "æ•ˆæœ", "æˆæ•ˆ", "åŸ·è¡Œæˆæœ"],
    "é€²å±•": ["é€²åº¦", "ç™¼å±•", "æ”¹å–„", "æå‡", "æˆé•·"],
    
    # çµ„ç¹”æ¶æ§‹ç›¸é—œ
    "ä¼æ¥­": ["å…¬å¸", "çµ„ç¹”", "æ©Ÿæ§‹", "é›†åœ˜", "äº‹æ¥­é«”", "æ³•äºº"],
    "å¯¦é«”": ["å–®ä½", "å­å…¬å¸", "é—œä¿‚ä¼æ¥­", "ç‡Ÿé‹æ“šé»", "äº‹æ¥­éƒ¨"],
    "æµç¨‹": ["ç¨‹åº", "ä½œæ¥­æµç¨‹", "å·¥ä½œæµç¨‹", "æ©Ÿåˆ¶", "åˆ¶åº¦"],
    "æ¶æ§‹": ["çµ„ç¹”", "é«”ç³»", "æ¡†æ¶", "çµæ§‹", "ç³»çµ±"],
    
    # æ°¸çºŒç™¼å±•ç›¸é—œ
    "æ°¸çºŒç›¸é—œ": ["æ°¸çºŒç™¼å±•", "å¯æŒçºŒ", "ESG", "ä¼æ¥­ç¤¾æœƒè²¬ä»»", "CSR"],
    "æ°¸çºŒ": ["å¯æŒçºŒ", "ESG", "ä¼æ¥­ç¤¾æœƒè²¬ä»»", "CSR", "æ°¸çºŒç™¼å±•"],
    "è½‰å‹": ["è½‰æ›", "è®Šé©", "æ”¹è®Š", "å‡ç´š", "æ›´æ–°", "é©æ–°"],
    "å‰µæ–°": ["ç ”ç™¼", "é–‹ç™¼", "æ”¹è‰¯", "çªç ´", "é€²æ­¥"],
    "å½±éŸ¿": ["è¡æ“Š", "æ•ˆæ‡‰", "ä½œç”¨", "çµæœ", "å¾Œæœ"],
    
    # è²¡å‹™ç›¸é—œ
    "è²¡å‹™": ["è²¡å‹™å ±è¡¨", "æœƒè¨ˆ", "ç¶“æ¿Ÿ", "è³‡é‡‘", "æŠ•è³‡"],
    "è³‡æœ¬": ["æŠ•è³‡", "è³‡é‡‘", "æˆæœ¬", "è²»ç”¨", "æ”¯å‡º"],
    "æ”¶ç›Š": ["ç‡Ÿæ”¶", "ç²åˆ©", "ç›ˆåˆ©", "æ”¶å…¥", "æ•ˆç›Š"],
    "æˆæœ¬": ["è²»ç”¨", "æ”¯å‡º", "æŠ•å…¥", "é–‹éŠ·", "ä»£åƒ¹"],
    
    # æ™‚é–“ç›¸é—œ
    "æœŸé–“": ["å¹´åº¦", "æœŸåˆ¥", "æ™‚æœŸ", "éšæ®µ", "é€±æœŸ"],
    "é•·æœŸ": ["ä¸­é•·æœŸ", "æœªä¾†", "é•·é ", "æŒçºŒ", "æ°¸çºŒ"],
    "çŸ­æœŸ": ["è¿‘æœŸ", "ç•¶æœŸ", "å³æ™‚", "ç›®å‰", "ç¾éšæ®µ"],
    "å®šæœŸ": ["ä¾‹è¡Œ", "å¸¸è¦", "é€±æœŸæ€§", "å›ºå®š", "æŒçºŒæ€§"]
}

# åœç”¨è©åˆ—è¡¨
STOP_WORDS = {
    "çš„", "å’Œ", "èˆ‡", "æˆ–", "åŠ", "ä»¥", "ç‚º", "åœ¨", "æ˜¯", "æœ‰", "å…¶", "è©²", "æ­¤", 
    "é€™", "é‚£", "å°‡", "å·²", "å¯", "èƒ½", "æœƒ", "æ‡‰", "é ˆ", "ç­‰", "æ–¼", "å¾", "å°", 
    "ç”±", "æ‰€", "è¢«", "è€Œ", "ä½†", "ç„¶", "å‰‡", "ä¹Ÿ", "éƒ½", "é‚„", "å°±", "åª", "åˆ",
    "æ›´", "æœ€", "å¾ˆ", "é", "ä¸", "æœª", "ç„¡", "æ²’", "ä¸¦", "ä¸”", "å¦‚", "è‹¥", "ä½¿"
}

# éœ€è¦æ’é™¤çš„æ¢æ–‡IDï¼ˆä¸å»ºç«‹ç´¢å¼•ï¼Œä¹Ÿä¸åƒèˆ‡åˆ†æï¼‰
EXCLUDED_ARTICLE_IDS = {"IFRS-S1-20"}

class HybridRetrievalSystem:
    """
    æ··åˆæª¢ç´¢ç³»çµ± - FAISSèªç¾©æœç´¢ + BM25é—œéµè©æœç´¢
    å¹³è¡¡çµ„åˆæ–¹æ¡ˆçš„æ ¸å¿ƒçµ„ä»¶
    """
    
    def __init__(self, vector_dim=768, cache_dir="./vector_cache"):
        self.vector_dim = vector_dim
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        # FAISSç´¢å¼•
        self.faiss_index = None
        self.document_embeddings = None
        self.documents = []
        self.article_mapping = {}
        
        # BM25çµ„ä»¶
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        
        # æ€§èƒ½çµ±è¨ˆ
        self.stats = {
            'total_queries': 0,
            'avg_retrieval_time': 0.0,
            'cache_hits': 0
        }
        
        logger.info(f"ğŸš€ åˆå§‹åŒ–æ··åˆæª¢ç´¢ç³»çµ± - å‘é‡ç¶­åº¦: {vector_dim}")

    def _filter_articles(self, articles: List[Dict]) -> List[Dict]:
        """åœ¨å»ºç«‹ç´¢å¼•å‰éæ¿¾æ‰è¢«æ’é™¤çš„æ¢æ–‡ã€‚"""
        if not articles:
            return articles
        filtered = [a for a in articles if a.get('id') not in EXCLUDED_ARTICLE_IDS]
        removed = len(articles) - len(filtered)
        if removed > 0:
            logger.info(f"â­ï¸ å·²æ’é™¤ {removed} æ¢æ¢æ–‡ï¼ˆä¾‹å¦‚: IFRS-S1-20ï¼‰ä¸å»ºç«‹ç´¢å¼•")
        return filtered
    
    def precompute_article_vectors(self, articles: List[Dict]) -> None:
        """
        é è¨ˆç®—æ‰€æœ‰IFRS S1æ¢æ–‡çš„å‘é‡è¡¨ç¤º
        é€™æ˜¯æ€§èƒ½å„ªåŒ–çš„é—œéµæ­¥é©Ÿ
        """
        start_time = time.time()
        # å…ˆéæ¿¾è¢«æ’é™¤çš„æ¢æ–‡
        articles = self._filter_articles(articles)
        logger.info(f"ğŸ“Š é–‹å§‹é è¨ˆç®— {len(articles)} æ¢IFRS S1æ¢æ–‡å‘é‡...")
        
        cache_path = self.cache_dir / "article_vectors.pkl"
        
        # æª¢æŸ¥ç·©å­˜
        if cache_path.exists():
            logger.info("âš¡ å¾ç·©å­˜è¼‰å…¥é è¨ˆç®—å‘é‡...")
            with open(cache_path, 'rb') as f:
                cache_data = pickle.load(f)
                self.faiss_index = cache_data['faiss_index']
                self.document_embeddings = cache_data['embeddings']
                self.documents = cache_data['documents']
                self.article_mapping = cache_data['mapping']
                self.tfidf_vectorizer = cache_data['tfidf_vectorizer']
                self.tfidf_matrix = cache_data['tfidf_matrix']
            # å¦‚æœç·©å­˜ä¸­ä»åŒ…å«è¢«æ’é™¤çš„æ¢æ–‡ï¼Œå‰‡é‡æ–°å»ºç«‹ç´¢å¼•
            try:
                contains_excluded = any(
                    (isinstance(v, dict) and v.get('id') in EXCLUDED_ARTICLE_IDS)
                    for v in self.article_mapping.values()
                )
            except Exception:
                contains_excluded = False

            if not contains_excluded:
                load_time = time.time() - start_time
                logger.info(f"âœ… ç·©å­˜è¼‰å…¥å®Œæˆ - è€—æ™‚: {load_time:.2f}ç§’")
                return
            else:
                logger.warning("â™»ï¸ ç·©å­˜åŒ…å«å·²æ’é™¤æ¢æ–‡ï¼ˆä¾‹å¦‚ IFRS-S1-20ï¼‰ï¼Œæ­£åœ¨å¿½ç•¥ç·©å­˜ä¸¦é‡æ–°å»ºç«‹ç´¢å¼•...")
        
        # æº–å‚™æ–‡æª”æ–‡æœ¬
        documents = []
        article_mapping = {}
        
        for idx, article in enumerate(articles):
            # çµ„åˆæ¨™é¡Œå’Œå…§å®¹ä½œç‚ºæœç´¢æ–‡æœ¬
            doc_text = f"{article.get('title', '')} {article.get('content', '')}"
            documents.append(doc_text)
            article_mapping[idx] = {
                'id': article.get('id'),
                'title': article.get('title'),
                'category': article.get('category'),
                'difficulty': article.get('difficulty', 'medium'),
                'keywords': article.get('keywords', [])
            }
        
        self.documents = documents
        self.article_mapping = article_mapping
        
        # ä½¿ç”¨çœŸå¯¦çš„èªç¾©åµŒå…¥æ¨¡å‹ï¼ˆSentence Transformers + MPSåŠ é€Ÿï¼‰
        logger.info("ğŸ”„ ç”Ÿæˆæ–‡æª”å‘é‡è¡¨ç¤º...")
        embeddings = self._generate_real_embeddings(documents)
        self.document_embeddings = embeddings
        
        # å»ºç«‹FAISSç´¢å¼•
        self._build_faiss_index(embeddings)
        
        # å»ºç«‹BM25ç´¢å¼•
        self._build_bm25_index(documents)
        
        # ä¿å­˜åˆ°ç·©å­˜
        cache_data = {
            'faiss_index': self.faiss_index,
            'embeddings': self.document_embeddings,
            'documents': self.documents,
            'mapping': self.article_mapping,
            'tfidf_vectorizer': self.tfidf_vectorizer,
            'tfidf_matrix': self.tfidf_matrix
        }
        
        with open(cache_path, 'wb') as f:
            pickle.dump(cache_data, f)
        
        total_time = time.time() - start_time
        logger.info(f"âœ… æ¢æ–‡å‘é‡é è¨ˆç®—å®Œæˆ - ç¸½è€—æ™‚: {total_time:.2f}ç§’")
    
    def _generate_real_embeddings(self, texts: List[str]) -> np.ndarray:
        """
        æ™ºèƒ½æ··åˆæ¨¡å¼ï¼šå°æ‰¹é‡ç”¨æ·±åº¦å­¸ç¿’ï¼Œå¤§æ‰¹é‡ç”¨TF-IDFå¿«é€Ÿæ¨¡å¼
        æ›¿æ›åŸæœ‰çš„TF-IDFæ¨¡æ“¬æ–¹æ³•
        """
        if not texts:
            return np.array([])
            
        # æ™ºèƒ½æ¨¡å¼é¸æ“‡ï¼šåªæœ‰å–®å€‹é …ç›®æˆ–2é …ç”¨æ·±åº¦å­¸ç¿’ï¼Œ3é …ä»¥ä¸Šç”¨TF-IDFå¿«é€Ÿæ¨¡å¼
        if len(texts) > 2:
            logger.info(f"ğŸƒ å¿«é€Ÿæ¨¡å¼ ({len(texts)}é …) - ä½¿ç”¨TF-IDFå¿«é€Ÿè™•ç†")
            return self._generate_fallback_embeddings(texts)
        
        logger.info(f"ğŸ§  æ·±åº¦å­¸ç¿’æ¨¡å¼ ({len(texts)}é …) - ä½¿ç”¨Sentence Transformers")
        
        try:
            # æª¢æŸ¥æ˜¯å¦å·²åˆå§‹åŒ–æ¨¡å‹
            if not hasattr(self, '_embedding_model'):
                logger.info("ğŸš€ æ­£åœ¨è¼‰å…¥Sentence Transformeræ¨¡å‹...")
                from sentence_transformers import SentenceTransformer
                import torch
                
                # æª¢æŸ¥MPSå¯ç”¨æ€§
                device = 'cpu'  # é è¨­ä½¿ç”¨CPU
                if torch.backends.mps.is_available():
                    device = 'mps'
                    logger.info("âœ… æª¢æ¸¬åˆ°MPSæ”¯æ´ï¼Œä½¿ç”¨Metal Performance ShadersåŠ é€Ÿ")
                elif torch.cuda.is_available():
                    device = 'cuda'
                    logger.info("âœ… æª¢æ¸¬åˆ°CUDAæ”¯æ´ï¼Œä½¿ç”¨GPUåŠ é€Ÿ")
                else:
                    logger.info("â„¹ï¸  ä½¿ç”¨CPUé‹ç®—")
                
                # è¼‰å…¥å¤šèªè¨€æ¨¡å‹ï¼ˆæ”¯æ´ä¸­è‹±æ–‡ï¼‰
                model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
                self._embedding_model = SentenceTransformer(model_name, device=device)
                self._device = device
                
                # èª¿æ•´å‘é‡ç¶­åº¦ï¼ˆè©²æ¨¡å‹è¼¸å‡º384ç¶­ï¼Œéœ€è¦åŒ¹é…self.vector_dim=768ï¼‰
                actual_dim = self._embedding_model.get_sentence_embedding_dimension()
                if actual_dim != self.vector_dim:
                    logger.info(f"ğŸ”§ æ¨¡å‹è¼¸å‡ºç¶­åº¦ {actual_dim}ï¼Œç›®æ¨™ç¶­åº¦ {self.vector_dim}")
                    if actual_dim < self.vector_dim:
                        # å¦‚æœæ¨¡å‹ç¶­åº¦å°æ–¼ç›®æ¨™ç¶­åº¦ï¼Œé€šéé‡è¤‡å’Œéš¨æ©Ÿå™ªè²æ“´å±•
                        self._dimension_adapter = 'expand'
                    else:
                        # å¦‚æœæ¨¡å‹ç¶­åº¦å¤§æ–¼ç›®æ¨™ç¶­åº¦ï¼Œé€šéPCAé™ç¶­
                        self._dimension_adapter = 'reduce'
                        from sklearn.decomposition import PCA
                        self._pca = PCA(n_components=self.vector_dim)
                else:
                    self._dimension_adapter = None
                
                logger.info(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œä½¿ç”¨è¨­å‚™: {device}")
            
            if not texts:
                return np.zeros((0, self.vector_dim), dtype=np.float32)
            
            # é è™•ç†æ–‡æœ¬
            processed_texts = []
            for text in texts:
                # æ¸…ç†ä¸¦æˆªæ–·æ–‡æœ¬
                clean_text = text.strip()
                if len(clean_text) > 512:  # é™åˆ¶é•·åº¦é¿å…å…§å­˜å•é¡Œ
                    clean_text = clean_text[:512]
                processed_texts.append(clean_text if clean_text else "ç©ºæ–‡æœ¬")
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            logger.info(f"ğŸ”„ æ­£åœ¨ç‚º {len(processed_texts)} å€‹æ–‡æœ¬ç”Ÿæˆèªç¾©åµŒå…¥...")
            embeddings = self._embedding_model.encode(
                processed_texts,
                convert_to_numpy=True,
                show_progress_bar=len(processed_texts) > 10,
                batch_size=32  # æ§åˆ¶æ‰¹æ¬¡å¤§å°
            )
            
            # ç¶­åº¦èª¿æ•´
            if self._dimension_adapter == 'expand':
                # æ“´å±•ç¶­åº¦ï¼šé‡è¤‡å‘é‡ä¸¦æ·»åŠ å°é‡éš¨æ©Ÿå™ªè²
                repeat_factor = self.vector_dim // embeddings.shape[1]
                remainder = self.vector_dim % embeddings.shape[1]
                
                expanded_embeddings = np.tile(embeddings, (1, repeat_factor))
                if remainder > 0:
                    expanded_embeddings = np.hstack([
                        expanded_embeddings,
                        embeddings[:, :remainder]
                    ])
                
                # æ·»åŠ å°é‡å™ªè²å¢åŠ å¤šæ¨£æ€§
                noise = np.random.normal(0, 0.01, expanded_embeddings.shape)
                embeddings = expanded_embeddings + noise
                
            elif self._dimension_adapter == 'reduce':
                # é™ç¶­ï¼šä½¿ç”¨PCA
                if not hasattr(self, '_pca_fitted'):
                    self._pca.fit(embeddings)
                    self._pca_fitted = True
                embeddings = self._pca.transform(embeddings)
            
            # ç¢ºä¿ç¶­åº¦æ­£ç¢º
            assert embeddings.shape[1] == self.vector_dim, f"ç¶­åº¦ä¸åŒ¹é…: {embeddings.shape[1]} != {self.vector_dim}"
            
            # æ­£è¦åŒ–å‘é‡ï¼ˆå°FAISSå…§ç©æœç´¢å¾ˆé‡è¦ï¼‰
            norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
            norms[norms == 0] = 1  # é¿å…é™¤é›¶
            embeddings = embeddings / norms
            
            logger.info(f"âœ… èªç¾©åµŒå…¥ç”Ÿæˆå®Œæˆ - å½¢ç‹€: {embeddings.shape}, è¨­å‚™: {self._device}")
            return embeddings.astype(np.float32)
            
        except ImportError as e:
            logger.warning(f"âš ï¸  Sentence Transformersæœªå®‰è£ï¼Œå›é€€åˆ°TF-IDFæ–¹æ³•: {e}")
            return self._generate_fallback_embeddings(texts)
        except Exception as e:
            logger.error(f"âŒ èªç¾©åµŒå…¥ç”Ÿæˆå¤±æ•—ï¼Œå›é€€åˆ°TF-IDFæ–¹æ³•: {e}")
            return self._generate_fallback_embeddings(texts)
    
    def _generate_fallback_embeddings(self, texts: List[str]) -> np.ndarray:
        """
        å‚™æ´åµŒå…¥æ–¹æ³• - TF-IDF + éš¨æ©ŸæŠ•å½±ï¼ˆåŸæœ‰æ–¹æ³•ï¼‰
        """
        logger.info("ğŸ”„ ä½¿ç”¨TF-IDFå‚™æ´æ–¹æ³•ç”ŸæˆåµŒå…¥...")
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.random_projection import SparseRandomProjection
        
        # TF-IDFå‘é‡åŒ–
        vectorizer = TfidfVectorizer(max_features=2048, stop_words='english')
        tfidf_vectors = vectorizer.fit_transform(texts)
        
        # é™ç¶­åˆ°ç›®æ¨™ç¶­åº¦
        if tfidf_vectors.shape[1] > self.vector_dim:
            projector = SparseRandomProjection(n_components=self.vector_dim)
            reduced_vectors = projector.fit_transform(tfidf_vectors)
        else:
            # å¦‚æœç¶­åº¦ä¸è¶³ï¼Œå¡«å……é›¶
            reduced_vectors = np.hstack([
                tfidf_vectors.toarray(),
                np.zeros((tfidf_vectors.shape[0], self.vector_dim - tfidf_vectors.shape[1]))
            ])
        
        return reduced_vectors.astype(np.float32)
    
    def _build_faiss_index(self, embeddings: np.ndarray) -> None:
        """å»ºç«‹FAISSç´¢å¼•ç”¨æ–¼å¿«é€Ÿç›¸ä¼¼åº¦æœç´¢"""
        logger.info("ğŸ”§ å»ºç«‹FAISSç´¢å¼•...")
        
        # å°æ–¼IFRS S1æ¢æ–‡æ•¸æ“šé›†ï¼ˆ126æ¢ï¼‰ï¼Œä½¿ç”¨ç°¡å–®å¹³é¢ç´¢å¼•æœ€åˆé©
        # é¿å…IVFèšé¡å•é¡Œï¼Œç¢ºä¿ç©©å®šæ€§
        self.faiss_index = faiss.IndexFlatIP(self.vector_dim)
        
        # æ·»åŠ å‘é‡åˆ°ç´¢å¼•
        self.faiss_index.add(embeddings)
        logger.info(f"âœ… FAISSç´¢å¼•å»ºç«‹å®Œæˆ - {len(embeddings)}å€‹æ¢æ–‡å‘é‡")
    
    def _build_bm25_index(self, documents: List[str]) -> None:
        """å»ºç«‹BM25/TF-IDFç´¢å¼•ç”¨æ–¼é—œéµè©æœç´¢"""
        logger.info("ğŸ”§ å»ºç«‹BM25ç´¢å¼•...")
        
        # ç°¡åŒ–çš„BM25å¯¦ç¾ï¼ˆä½¿ç”¨TF-IDFè¿‘ä¼¼ï¼‰
        # æ ¹æ“šæ–‡æª”æ•¸é‡èª¿æ•´åƒæ•¸
        min_df = min(1, len(documents) // 4) if len(documents) > 2 else 1
        max_df = 0.95 if len(documents) > 10 else 1.0
        
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=min(5000, len(documents) * 100),
            stop_words='english',
            ngram_range=(1, 2),
            max_df=max_df,
            min_df=min_df
        )
        
        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(documents)
        logger.info(f"âœ… BM25ç´¢å¼•å»ºç«‹å®Œæˆ - è©å½™é‡: {len(self.tfidf_vectorizer.vocabulary_)}")
    
    def hybrid_search(self, query: str, top_k: int = 10, semantic_weight: float = 0.7) -> List[Dict]:
        """
        æ··åˆæœç´¢ï¼šçµåˆèªç¾©æœç´¢å’Œé—œéµè©æœç´¢
        
        Args:
            query: æœç´¢æŸ¥è©¢
            top_k: è¿”å›çµæœæ•¸é‡
            semantic_weight: èªç¾©æœç´¢æ¬Šé‡ (0.0-1.0)
            
        Returns:
            æ’åºå¾Œçš„æœç´¢çµæœ
        """
        start_time = time.time()
        
        # èªç¾©æœç´¢åˆ†æ•¸
        semantic_scores = self._semantic_search(query, top_k * 2)
        
        # é—œéµè©æœç´¢åˆ†æ•¸  
        keyword_scores = self._keyword_search(query, top_k * 2)
        
        # æ··åˆè©•åˆ†
        hybrid_scores = self._combine_scores(
            semantic_scores, 
            keyword_scores, 
            semantic_weight
        )
        
        # æ’åºä¸¦è¿”å›top_kçµæœ
        results = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]
        
        # æ§‹å»ºçµæœ
        search_results = []
        for doc_idx, score in results:
            article_info = self.article_mapping[doc_idx]
            search_results.append({
                'article_id': article_info['id'],
                'title': article_info['title'],
                'category': article_info['category'],
                'difficulty': article_info['difficulty'],
                'score': float(score),
                'doc_index': doc_idx
            })
        
        # æ›´æ–°çµ±è¨ˆ
        retrieval_time = time.time() - start_time
        self.stats['total_queries'] += 1
        self.stats['avg_retrieval_time'] = (
            (self.stats['avg_retrieval_time'] * (self.stats['total_queries'] - 1) + retrieval_time) 
            / self.stats['total_queries']
        )
        
        logger.info(f"ğŸ” æ··åˆæœç´¢å®Œæˆ - è€—æ™‚: {retrieval_time:.3f}ç§’, çµæœæ•¸: {len(search_results)}")
        return search_results
    
    def _semantic_search(self, query: str, k: int) -> Dict[int, float]:
        """èªç¾©å‘é‡æœç´¢"""
        # ç”ŸæˆæŸ¥è©¢å‘é‡
        query_vector = self._generate_real_embeddings([query])[0:1]
        
        # FAISSæœç´¢
        scores, indices = self.faiss_index.search(query_vector, k)
        
        # è½‰æ›ç‚ºå­—å…¸æ ¼å¼
        semantic_scores = {}
        for score, idx in zip(scores[0], indices[0]):
            if idx != -1:  # æœ‰æ•ˆç´¢å¼•
                semantic_scores[idx] = float(score)
        
        return semantic_scores
    
    def _keyword_search(self, query: str, k: int) -> Dict[int, float]:
        """é—œéµè©TF-IDFæœç´¢"""
        # å‘é‡åŒ–æŸ¥è©¢
        query_vector = self.tfidf_vectorizer.transform([query])
        
        # è¨ˆç®—ç›¸ä¼¼åº¦
        similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()
        
        # ç²å–top-k
        top_indices = np.argsort(similarities)[-k:][::-1]
        
        keyword_scores = {}
        for idx in top_indices:
            if similarities[idx] > 0:
                keyword_scores[idx] = float(similarities[idx])
        
        return keyword_scores
    
    def _combine_scores(self, semantic_scores: Dict[int, float], 
                       keyword_scores: Dict[int, float], 
                       semantic_weight: float) -> Dict[int, float]:
        """çµ„åˆèªç¾©å’Œé—œéµè©åˆ†æ•¸"""
        all_docs = set(semantic_scores.keys()) | set(keyword_scores.keys())
        
        # æ­£è¦åŒ–åˆ†æ•¸
        if semantic_scores:
            max_semantic = max(semantic_scores.values())
            semantic_scores = {k: v/max_semantic for k, v in semantic_scores.items()}
        
        if keyword_scores:
            max_keyword = max(keyword_scores.values())
            keyword_scores = {k: v/max_keyword for k, v in keyword_scores.items()}
        
        # åŠ æ¬Šçµ„åˆ
        combined_scores = {}
        for doc_idx in all_docs:
            semantic_score = semantic_scores.get(doc_idx, 0.0)
            keyword_score = keyword_scores.get(doc_idx, 0.0)
            
            combined_score = (
                semantic_weight * semantic_score + 
                (1 - semantic_weight) * keyword_score
            )
            combined_scores[doc_idx] = combined_score
        
        return combined_scores
    
    def get_stats(self) -> Dict:
        """ç²å–æ€§èƒ½çµ±è¨ˆ"""
        return {
            **self.stats,
            'indexed_documents': len(self.documents),
            'vector_dimension': self.vector_dim,
            'faiss_index_size': self.faiss_index.ntotal if self.faiss_index else 0
        }

class FastIFRSAnalyzer:
    """
    å¿«é€ŸIFRS S1åˆ†æå™¨
    ä½¿ç”¨FAISS+æ··åˆæª¢ç´¢å¯¦ç¾5-8åˆ†é˜å…§å®Œæˆåˆ†æ
    """
    
    def __init__(self):
        self.retrieval_system = HybridRetrievalSystem()
        self.initialized = False
        
    def initialize_with_articles(self, articles: List[Dict]) -> None:
        """ä½¿ç”¨IFRS S1æ¢æ–‡åˆå§‹åŒ–ç³»çµ±"""
        if not self.initialized:
            logger.info("ğŸš€ åˆå§‹åŒ–å¿«é€ŸIFRS S1åˆ†æç³»çµ±...")
            self.retrieval_system.precompute_article_vectors(articles)
            self.initialized = True
            logger.info("âœ… ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
    
    def analyze_document_fast(self, document_text: str, 
                            sentences: List[str], 
                            paragraphs: List[str]) -> List[Dict]:
        """
        å¿«é€Ÿæ–‡æª”åˆ†æ - æ ¸å¿ƒå„ªåŒ–å‡½æ•¸
        
        Args:
            document_text: å®Œæ•´æ–‡æª”æ–‡æœ¬
            sentences: æ–‡æª”å¥å­åˆ—è¡¨
            paragraphs: æ–‡æª”æ®µè½åˆ—è¡¨
            
        Returns:
            åˆ†æçµæœåˆ—è¡¨
        """
        if not self.initialized:
            raise ValueError("ç³»çµ±æœªåˆå§‹åŒ–ï¼Œè«‹å…ˆèª¿ç”¨ initialize_with_articles()")
        
        start_time = time.time()
        logger.info(f"ğŸ” é–‹å§‹å¿«é€Ÿæ–‡æª”åˆ†æ - å¥å­æ•¸: {len(sentences)}, æ®µè½æ•¸: {len(paragraphs)}")
        
        results = []
        
        # å‹•æ…‹æ‰¹é‡è™•ç†
        batch_size = min(50, len(sentences) + len(paragraphs))
        all_text_chunks = sentences + paragraphs
        
        for i in range(0, len(all_text_chunks), batch_size):
            batch_chunks = all_text_chunks[i:i+batch_size]
            batch_query = " ".join(batch_chunks[:3])  # ä½¿ç”¨å‰3å€‹ä½œç‚ºæŸ¥è©¢
            
            # æ··åˆæª¢ç´¢åŒ¹é…æ¢æ–‡
            matched_articles = self.retrieval_system.hybrid_search(
                batch_query, 
                top_k=20,  # æ¸›å°‘å€™é¸æ•¸é‡
                semantic_weight=0.7
            )
            
            # è™•ç†åŒ¹é…çµæœ
            for article in matched_articles:
                if article['score'] > 0.3:  # æé«˜é–¾å€¼ä»¥ç²å¾—é«˜è³ªé‡çµæœ
                    # å¿«é€Ÿè­‰æ“šåŒ¹é…
                    evidences = self._fast_evidence_matching(
                        batch_chunks, article, max_evidences=3
                    )
                    
                    if evidences:
                        results.append({
                            'article': article,
                            'evidences': evidences,
                            'confidence': article['score']
                        })
        
        # å»é‡ä¸¦æ’åº
        results = self._deduplicate_results(results)
        
        analysis_time = time.time() - start_time
        logger.info(f"âš¡ å¿«é€Ÿåˆ†æå®Œæˆ - è€—æ™‚: {analysis_time:.2f}ç§’, çµæœæ•¸: {len(results)}")
        
        return results
    
    def analyze_article_specific(self, article: Dict, document_sentences: List[str], 
                               document_paragraphs: List[str]) -> Dict:
        """
        æ¢æ–‡ç‰¹å®šåˆ†æ - é‡å°å–®å€‹IFRSæ¢æ–‡é€²è¡Œç²¾ç¢ºåŒ¹é…
        
        Args:
            article: å–®å€‹IFRSæ¢æ–‡å­—å…¸
            document_sentences: æ–‡æª”å¥å­åˆ—è¡¨ 
            document_paragraphs: æ–‡æª”æ®µè½åˆ—è¡¨
            
        Returns:
            è©²æ¢æ–‡çš„åˆ†æçµæœå­—å…¸
        """
        if not self.initialized:
            raise ValueError("ç³»çµ±æœªåˆå§‹åŒ–ï¼Œè«‹å…ˆèª¿ç”¨ initialize_with_articles()")
        
        # è‹¥ç‚ºè¢«æ’é™¤æ¢æ–‡ï¼Œç›´æ¥è·³éåˆ†æ
        if article.get('id') in EXCLUDED_ARTICLE_IDS:
            logger.info(f"â­ï¸ è·³éæ’é™¤æ¢æ–‡: {article.get('id')}")
            return {
                'article_id': article.get('id'),
                'article_data': article,
                'evidences': [],
                'max_similarity': 0.0,
                'processing_time': 0.0,
                'skipped': True
            }
        
        start_time = time.time()
        logger.debug(f"ğŸ¯ é–‹å§‹æ¢æ–‡ç‰¹å®šåˆ†æ: {article.get('id', 'Unknown')}")
        
        # æ§‹å»ºæ¢æ–‡ç‰¹å®šçš„æŸ¥è©¢æ–‡æœ¬
        article_query = f"{article.get('title', '')} {article.get('content', '')} {' '.join(article.get('keywords', []))}"
        
        # ä½¿ç”¨æ¢æ–‡å…§å®¹é€²è¡Œèªç¾©æª¢ç´¢
        all_document_chunks = document_sentences + document_paragraphs
        
        # è¨ˆç®—èˆ‡æ–‡æª”å„éƒ¨åˆ†çš„ç›¸ä¼¼åº¦
        similarities = self._calculate_specific_similarities(article_query, all_document_chunks)
        logger.debug(f"ğŸ¯ ç›¸ä¼¼åº¦è¨ˆç®—çµæœ: {len(similarities)} å€‹åˆ†æ•¸, æœ€é«˜: {max(similarities) if similarities else 0:.4f}")
        
        # å‹•æ…‹é–¾å€¼ç­–ç•¥
        dynamic_threshold = self._calculate_dynamic_threshold(similarities, article)
        
        # åŒ¹é…é«˜ç›¸ä¼¼åº¦çš„å¥å­å’Œæ®µè½
        sentence_matches = []
        paragraph_matches = []
        
        # è™•ç†å¥å­åŒ¹é… - ä½¿ç”¨å‹•æ…‹é–¾å€¼
        for i, similarity in enumerate(similarities[:len(document_sentences)]):
            if similarity > dynamic_threshold:
                # å†æ¬¡æª¢æŸ¥å…§å®¹è³ªé‡
                if not self._is_irrelevant_content(document_sentences[i]):
                    sentence_matches.append({
                        'content': document_sentences[i],
                        'similarity': similarity,
                        'type': 'sentence',
                        'index': i,
                        'page': (i // 20) + 1  # ä¼°ç®—é ç¢¼ï¼Œå‡è¨­æ¯é ç´„20å€‹å¥å­
                    })
        
        # è™•ç†æ®µè½åŒ¹é… - æ®µè½é–¾å€¼ç¨å¾®æ”¾å¯¬
        paragraph_threshold = dynamic_threshold * 0.8
        for i, similarity in enumerate(similarities[len(document_sentences):]):
            if similarity > paragraph_threshold:
                # å†æ¬¡æª¢æŸ¥å…§å®¹è³ªé‡
                if not self._is_irrelevant_content(document_paragraphs[i]):
                    paragraph_matches.append({
                        'content': document_paragraphs[i],
                        'similarity': similarity,
                        'type': 'paragraph', 
                        'index': len(document_sentences) + i,
                        'page': (i // 5) + 1  # ä¼°ç®—é ç¢¼ï¼Œå‡è¨­æ¯é ç´„5å€‹æ®µè½
                    })
        
        # åˆä½µä¸¦æ’åºè­‰æ“š
        all_evidences = sentence_matches + paragraph_matches
        all_evidences.sort(key=lambda x: x['similarity'], reverse=True)
        
        # æ™ºèƒ½è­‰æ“šé¸æ“‡ - ç¢ºä¿æ¯å€‹æ¢æ–‡è‡³å°‘æœ‰1-2å€‹è­‰æ“š
        top_evidences = self._select_best_evidences(all_evidences, similarities)
        
        analysis_time = time.time() - start_time
        logger.debug(f"ğŸ¯ æ¢æ–‡åˆ†æå®Œæˆ: {article.get('id')} - è€—æ™‚: {analysis_time:.3f}ç§’, è­‰æ“šæ•¸: {len(top_evidences)}")
        
        return {
            'article_id': article.get('id'),
            'article_data': article,
            'evidences': top_evidences,
            'max_similarity': max([e['similarity'] for e in top_evidences], default=0.0),
            'processing_time': analysis_time
        }
    
    def _calculate_specific_similarities(self, query: str, document_chunks: List[str]) -> List[float]:
        """
        è¨ˆç®—æŸ¥è©¢èˆ‡æ–‡æª”ç‰‡æ®µçš„çœŸå¯¦èªç¾©ç›¸ä¼¼åº¦
        ä½¿ç”¨Sentence Transformersæ¨¡å‹é€²è¡Œç²¾ç¢ºè¨ˆç®—
        
        Args:
            query: æ¢æ–‡æŸ¥è©¢æ–‡æœ¬
            document_chunks: æ–‡æª”ç‰‡æ®µåˆ—è¡¨
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•¸åˆ—è¡¨
        """
        try:
            if not document_chunks or not query.strip():
                logger.debug(f"ç©ºè¼¸å…¥: query='{query}', chunks={len(document_chunks)}")
                return [0.0] * len(document_chunks)
            
            # ä½¿ç”¨çœŸå¯¦èªç¾©åµŒå…¥è¨ˆç®—ç›¸ä¼¼åº¦
            query_embedding = self.retrieval_system._generate_real_embeddings([query])
            chunk_embeddings = self.retrieval_system._generate_real_embeddings(document_chunks)
            
            # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
            from sklearn.metrics.pairwise import cosine_similarity
            similarities = cosine_similarity(query_embedding, chunk_embeddings)[0]
            
            # è½‰æ›ç‚ºåˆ—è¡¨ä¸¦ç¢ºä¿æ•¸å€¼ç¯„åœ
            similarities = [max(0.0, min(1.0, float(sim))) for sim in similarities]
            
            logger.debug(f"çœŸå¯¦èªç¾©ç›¸ä¼¼åº¦è¨ˆç®—å®Œæˆ: æœ€é«˜åˆ†={max(similarities):.4f}, æœ‰æ•ˆåŒ¹é…={sum(1 for s in similarities if s > 0.1)}")
            return similarities
            
        except Exception as e:
            logger.error(f"çœŸå¯¦èªç¾©ç›¸ä¼¼åº¦è¨ˆç®—å¤±æ•—ï¼Œå›é€€åˆ°è©å½™åŒ¹é…: {e}")
            # å›é€€åˆ°åŸæœ‰çš„è©å½™åŒ¹é…æ–¹æ³•
            return self._calculate_fallback_similarities(query, document_chunks)
    
    def _calculate_fallback_similarities(self, query: str, document_chunks: List[str]) -> List[float]:
        """
        å‚™æ´ç›¸ä¼¼åº¦è¨ˆç®—æ–¹æ³• - ä½¿ç”¨è©å½™åŒ¹é…
        """
        try:
            # æ–‡æœ¬é è™•ç†å’Œè©å½™æ¨™æº–åŒ–
            query_normalized = self._normalize_text(query)
            chunks_normalized = [self._normalize_text(chunk) for chunk in document_chunks]
            
            similarities = []
            for chunk_norm in chunks_normalized:
                if not chunk_norm:
                    similarities.append(0.0)
                    continue
                
                # èªç¾©æ„ŸçŸ¥åŒ¹é…
                similarity = self._semantic_similarity(query_normalized, chunk_norm)
                similarities.append(similarity)
            
            logger.debug(f"å‚™æ´ç›¸ä¼¼åº¦è¨ˆç®—å®Œæˆ: æœ€é«˜åˆ†={max(similarities):.4f}, æœ‰æ•ˆåŒ¹é…={sum(1 for s in similarities if s > 0.1)}")
            return similarities
            
        except Exception as e:
            logger.error(f"å‚™æ´ç›¸ä¼¼åº¦è¨ˆç®—å¤±æ•—: {e}")
            return [0.0] * len(document_chunks)
    
    def _normalize_text(self, text: str) -> str:
        """
        æ–‡æœ¬æ¨™æº–åŒ–è™•ç†
        """
        if not text.strip():
            return ""
        
        # æª¢æŸ¥ä¸¦éæ¿¾ç„¡é—œå…§å®¹
        if self._is_irrelevant_content(text):
            return ""
        
        # åŸºç¤æ¸…ç†
        text = text.strip().lower()
        
        # ç§»é™¤æ¨™é»ç¬¦è™Ÿä½†ä¿ç•™ä¸­æ–‡
        text = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', text)
        
        # çµ±ä¸€ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        
        return text
    
    def _is_irrelevant_content(self, text: str) -> bool:
        """
        åˆ¤æ–·æ˜¯å¦ç‚ºç„¡é—œçš„æ ¼å¼åŒ–å…§å®¹
        """
        if not text or len(text.strip()) < 10:
            return True
        
        text_lower = text.lower()
        
        # æª¢æŸ¥çµ„ç¹”åœ–è¡¨ç›¸é—œå…§å®¹
        org_chart_patterns = [
            r'â–ª.*â–ª.*â–ª',  # å¤šå€‹é …ç›®ç¬¦è™Ÿ
            r'^\d+\.\d+\s+\w+',  # ç« ç¯€ç·¨è™Ÿé–‹é ­
            r'è©³\s*\d+\.\d+',  # "è©³ 2.3" ç­‰åƒè€ƒ
            r'æ¯å¹´.*æ¬¡.*å‘ˆå ±',  # çµ„ç¹”æµç¨‹æè¿°
            r'å§”å“¡æœƒ.*å§”å“¡æœƒ.*å§”å“¡æœƒ',  # å¤šå€‹å§”å“¡æœƒé‡è¤‡
            r'åƒ¹å€¼.*åƒ¹å€¼.*åƒ¹å€¼',  # å¤šå€‹åƒ¹å€¼é‡è¤‡
        ]
        
        for pattern in org_chart_patterns:
            if re.search(pattern, text):
                return True
        
        # æª¢æŸ¥éå¤šçš„çµ„ç¹”å–®ä½åˆ—èˆ‰
        org_units = ['è™•', 'å®¤', 'éƒ¨', 'å» ', 'å§”å“¡æœƒ', 'å°çµ„']
        org_count = sum(text.count(unit) for unit in org_units)
        if org_count > 5:  # è¶…é5å€‹çµ„ç¹”å–®ä½å¯èƒ½æ˜¯çµ„ç¹”åœ–
            return True
        
        # æª¢æŸ¥æ˜¯å¦ä¸»è¦æ˜¯é …ç›®ç¬¦è™Ÿå’Œçµ„ç¹”åç¨±
        bullet_points = text.count('â–ª') + text.count('â—') + text.count('â€¢')
        if bullet_points > 3 and len(text.split()) < bullet_points * 3:
            return True
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºç´”æ•¸å­—ç·¨è™Ÿå…§å®¹
        if re.match(r'^[\d\.\s]+$', text):
            return True
        
        # æª¢æŸ¥ç„¡æ„ç¾©çš„é‡è¤‡è©å½™
        words = text.split()
        if len(words) > 0:
            unique_words = set(words)
            if len(unique_words) / len(words) < 0.3:  # é‡è¤‡åº¦éé«˜
                return True
        
        return False
    
    def _semantic_similarity(self, query: str, chunk: str) -> float:
        """
        èªç¾©æ„ŸçŸ¥ç›¸ä¼¼åº¦è¨ˆç®—
        """
        if not query or not chunk:
            return 0.0
        
        # åˆ†è©è™•ç†
        query_words = self._tokenize_and_expand(query)
        chunk_words = self._tokenize_and_expand(chunk)
        
        if not query_words or not chunk_words:
            return 0.0
        
        # è¨ˆç®—åŠ æ¬ŠåŒ¹é…åˆ†æ•¸
        total_score = 0.0
        query_word_count = len(query_words)
        
        for query_word in query_words:
            max_word_score = 0.0
            
            for chunk_word in chunk_words:
                word_score = self._word_similarity(query_word, chunk_word)
                max_word_score = max(max_word_score, word_score)
            
            total_score += max_word_score
        
        # æ­£è¦åŒ–åˆ†æ•¸
        similarity = total_score / query_word_count if query_word_count > 0 else 0.0
        
        return min(similarity, 1.0)  # ç¢ºä¿ä¸è¶…é1.0
    
    def _tokenize_and_expand(self, text: str) -> List[str]:
        """
        åˆ†è©ä¸¦é€²è¡Œè©å½™æ“´å±•
        """
        if not text:
            return []
        
        # ç°¡å–®åˆ†è©
        words = text.split()
        
        # ç§»é™¤åœç”¨è©
        words = [word for word in words if word not in STOP_WORDS]
        
        # è©å½™æ“´å±• - æ·»åŠ åŒç¾©è©
        expanded_words = []
        for word in words:
            expanded_words.append(word)
            # æ·»åŠ æ˜ å°„è©å½™
            if word in VOCABULARY_MAPPING:
                expanded_words.extend(VOCABULARY_MAPPING[word])
        
        return expanded_words
    
    def _word_similarity(self, word1: str, word2: str) -> float:
        """
        å–®è©ç›¸ä¼¼åº¦è¨ˆç®—
        """
        if not word1 or not word2:
            return 0.0
        
        # ç²¾ç¢ºåŒ¹é…
        if word1 == word2:
            return 1.0
        
        # æª¢æŸ¥é›™å‘æ˜ å°„è©å½™
        score = self._check_vocabulary_mapping(word1, word2)
        if score > 0:
            return score
        
        # æª¢æŸ¥å­—ç¬¦ä¸²åŒ…å«é—œä¿‚
        if word1 in word2 or word2 in word1:
            longer = max(word1, word2, key=len)
            shorter = min(word1, word2, key=len)
            if len(shorter) >= 2:  # é¿å…å¤ªçŸ­çš„éƒ¨åˆ†åŒ¹é…
                return 0.6 * (len(shorter) / len(longer))
        
        return 0.0
    
    def _check_vocabulary_mapping(self, word1: str, word2: str) -> float:
        """
        æª¢æŸ¥è©å½™æ˜ å°„é—œä¿‚
        """
        # æª¢æŸ¥ word1 -> word2 çš„æ˜ å°„
        if word1 in VOCABULARY_MAPPING and word2 in VOCABULARY_MAPPING[word1]:
            return 0.8
        
        # æª¢æŸ¥ word2 -> word1 çš„æ˜ å°„
        if word2 in VOCABULARY_MAPPING and word1 in VOCABULARY_MAPPING[word2]:
            return 0.8
        
        # æª¢æŸ¥åå‘æ˜ å°„ï¼šword2 æ˜¯å¦åœ¨ä»»ä½•æ˜ å°„åˆ—è¡¨ä¸­ï¼Œä¸”å°æ‡‰çš„keyèˆ‡word1åŒ¹é…
        for key, values in VOCABULARY_MAPPING.items():
            if word2 in values and word1 == key:
                return 0.8
            if word1 in values and word2 == key:
                return 0.8
        
        return 0.0
    
    def _calculate_dynamic_threshold(self, similarities: List[float], article: Dict) -> float:
        """
        æ ¹æ“šæ¢æ–‡é¡åˆ¥å’Œç›¸ä¼¼åº¦åˆ†å¸ƒè¨ˆç®—å‹•æ…‹é–¾å€¼
        """
        if not similarities:
            return 0.1
        
        valid_similarities = [s for s in similarities if s > 0]
        if not valid_similarities:
            return 0.1
        
        # åŸºç¤é–¾å€¼æ ¹æ“šæ¢æ–‡é¡åˆ¥æ±ºå®š
        category = article.get('category', '').lower()
        base_thresholds = {
            'æ²»ç†': 0.15,      # æ²»ç†é¡åˆ¥ç”¨è©å·®ç•°è¼ƒå¤§ï¼Œé–¾å€¼è¼ƒä½
            'ç­–ç•¥': 0.20,      # ç­–ç•¥é¡åˆ¥ä¸­ç­‰
            'é¢¨éšªç®¡ç†': 0.18,   # é¢¨éšªç®¡ç†ä¸­ç­‰
            'æŒ‡æ¨™èˆ‡ç›®æ¨™': 0.25, # æŒ‡æ¨™é¡åˆ¥è¦æ±‚è¼ƒç²¾ç¢º
            'å ±å‘ŠåŸºç¤': 0.15   # å ±å‘ŠåŸºç¤è¼ƒå¯¬é¬†
        }
        
        base_threshold = 0.2  # é è¨­é–¾å€¼
        for cat_key, threshold in base_thresholds.items():
            if cat_key in category:
                base_threshold = threshold
                break
        
        # æ ¹æ“šç›¸ä¼¼åº¦åˆ†å¸ƒèª¿æ•´
        max_sim = max(valid_similarities)
        avg_sim = sum(valid_similarities) / len(valid_similarities)
        
        # å¦‚æœæœ€é«˜ç›¸ä¼¼åº¦å¾ˆä½ï¼Œé™ä½é–¾å€¼
        if max_sim < 0.3:
            base_threshold *= 0.7
        
        # å¦‚æœå¹³å‡ç›¸ä¼¼åº¦å¾ˆä½ï¼Œé€²ä¸€æ­¥é™ä½é–¾å€¼
        if avg_sim < 0.1:
            base_threshold *= 0.5
        
        # ç¢ºä¿é–¾å€¼åœ¨åˆç†ç¯„åœå…§
        final_threshold = max(0.05, min(base_threshold, 0.4))
        
        logger.debug(f"å‹•æ…‹é–¾å€¼è¨ˆç®—: é¡åˆ¥={category}, åŸºç¤={base_threshold:.3f}, æœ€çµ‚={final_threshold:.3f}")
        return final_threshold
    
    def _select_best_evidences(self, all_evidences: List[Dict], similarities: List[float]) -> List[Dict]:
        """
        æ™ºèƒ½é¸æ“‡æœ€ä½³è­‰æ“šï¼Œå»é™¤é‡ç–Šä¸¦ç¢ºä¿å¤šæ¨£æ€§
        """
        if not all_evidences and similarities:
            # å¦‚æœæ²’æœ‰è¶…éé–¾å€¼çš„è­‰æ“šï¼Œé¸æ“‡æœ€é«˜åˆ†çš„å‰1-2å€‹
            max_sim = max(similarities)
            if max_sim > 0.05:  # æœ€ä½åº•ç·š
                # æ‰¾åˆ°æœ€é«˜åˆ†çš„ç´¢å¼•
                max_indices = []
                for i, sim in enumerate(similarities):
                    if abs(sim - max_sim) < 1e-6:  # æµ®é»æ•¸æ¯”è¼ƒ
                        max_indices.append(i)
                
                # ç‚ºæœ€é«˜åˆ†é …ç›®å‰µå»ºè­‰æ“š
                backup_evidences = []
                for idx in max_indices[:2]:  # æœ€å¤šå–2å€‹
                    if idx < len(similarities):
                        backup_evidences.append({
                            'content': f"æœªæ‰¾åˆ°é«˜åŒ¹é…åº¦è­‰æ“šï¼Œé¡¯ç¤ºæœ€ç›¸é—œå…§å®¹ (ç›¸ä¼¼åº¦: {similarities[idx]:.3f})",
                            'similarity': similarities[idx],
                            'type': 'fallback',
                            'index': idx
                        })
                
                logger.debug(f"ä½¿ç”¨å‚™æ´è­‰æ“š: {len(backup_evidences)} å€‹, æœ€é«˜åˆ†: {max_sim:.3f}")
                return backup_evidences
        
        # å…ˆå»é™¤é‡ç–Šè­‰æ“š
        deduplicated_evidences = self._remove_overlapping_evidences(all_evidences)
        
        # é™åˆ¶è­‰æ“šæ•¸é‡ï¼Œå„ªå…ˆé¸æ“‡é«˜è³ªé‡è­‰æ“š
        max_evidences = 5
        if len(deduplicated_evidences) <= max_evidences:
            return deduplicated_evidences
        
        # å¤šæ¨£åŒ–é¸æ“‡ï¼šé é¢åˆ†æ•£ + é¡å‹å¹³è¡¡
        selected = self._diversified_evidence_selection(deduplicated_evidences, max_evidences)
        
        logger.debug(f"è­‰æ“šé¸æ“‡å®Œæˆ: {len(selected)} å€‹, å»é‡å‰: {len(all_evidences)}, å»é‡å¾Œ: {len(deduplicated_evidences)}")
        return selected
    
    def _remove_overlapping_evidences(self, evidences: List[Dict]) -> List[Dict]:
        """
        ç§»é™¤é‡ç–Šçš„è­‰æ“šå…§å®¹
        """
        if len(evidences) <= 1:
            return evidences
        
        deduplicated = []
        
        for current in evidences:
            current_content = current.get('content', '').strip()
            if not current_content:
                continue
                
            is_overlapping = False
            
            for existing in deduplicated:
                existing_content = existing.get('content', '').strip()
                if not existing_content:
                    continue
                
                # æª¢æŸ¥å…§å®¹é‡ç–Š
                overlap_ratio = self._calculate_content_overlap(current_content, existing_content)
                
                if overlap_ratio > 0.7:  # 70%ä»¥ä¸Šé‡ç–Šè¦–ç‚ºé‡è¤‡
                    is_overlapping = True
                    # ä¿ç•™ç›¸ä¼¼åº¦æ›´é«˜çš„è­‰æ“š
                    if current.get('similarity', 0) > existing.get('similarity', 0):
                        deduplicated.remove(existing)
                        deduplicated.append(current)
                    break
            
            if not is_overlapping:
                deduplicated.append(current)
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        deduplicated.sort(key=lambda x: x.get('similarity', 0), reverse=True)
        
        logger.debug(f"å»é‡å®Œæˆ: åŸå§‹ {len(evidences)} â†’ å»é‡å¾Œ {len(deduplicated)}")
        return deduplicated
    
    def _calculate_content_overlap(self, content1: str, content2: str) -> float:
        """
        è¨ˆç®—å…©å€‹å…§å®¹çš„é‡ç–Šæ¯”ä¾‹
        """
        if not content1 or not content2:
            return 0.0
        
        # ç°¡åŒ–è™•ç†ï¼šè¨ˆç®—å­—ç¬¦ç´šé‡ç–Š
        shorter_content = content1 if len(content1) <= len(content2) else content2
        longer_content = content2 if len(content1) <= len(content2) else content1
        
        # æª¢æŸ¥çŸ­æ–‡æœ¬æ˜¯å¦è¢«é•·æ–‡æœ¬åŒ…å«
        if shorter_content in longer_content:
            return 1.0
        
        # è¨ˆç®—è©ç´šé‡ç–Š
        words1 = set(content1.split())
        words2 = set(content2.split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        return intersection / union if union > 0 else 0.0
    
    def _diversified_evidence_selection(self, evidences: List[Dict], max_count: int) -> List[Dict]:
        """
        å¤šæ¨£åŒ–è­‰æ“šé¸æ“‡ï¼šç¢ºä¿é é¢åˆ†æ•£å’Œé¡å‹å¹³è¡¡
        """
        if len(evidences) <= max_count:
            return evidences
        
        selected = []
        used_pages = set()
        sentence_count = 0
        paragraph_count = 0
        
        # ç¬¬ä¸€è¼ªï¼šé¸æ“‡ä¸åŒé é¢çš„é«˜åˆ†è­‰æ“š
        for evidence in evidences:
            if len(selected) >= max_count:
                break
                
            page = evidence.get('page', evidence.get('index', 0))
            evidence_type = evidence.get('type', 'unknown')
            
            # å„ªå…ˆé¸æ“‡ä¸åŒé é¢çš„è­‰æ“š
            if page not in used_pages:
                if evidence_type == 'sentence' and sentence_count < max_count // 2 + 1:
                    selected.append(evidence)
                    used_pages.add(page)
                    sentence_count += 1
                elif evidence_type == 'paragraph' and paragraph_count < max_count // 2 + 1:
                    selected.append(evidence)
                    used_pages.add(page)
                    paragraph_count += 1
        
        # ç¬¬äºŒè¼ªï¼šå¦‚æœé‚„æœ‰ç©ºä½ï¼Œé¸æ“‡å‰©é¤˜çš„é«˜åˆ†è­‰æ“š
        for evidence in evidences:
            if len(selected) >= max_count:
                break
                
            if evidence not in selected:
                evidence_type = evidence.get('type', 'unknown')
                
                if evidence_type == 'sentence' and sentence_count < max_count // 2 + 1:
                    selected.append(evidence)
                    sentence_count += 1
                elif evidence_type == 'paragraph' and paragraph_count < max_count // 2 + 1:
                    selected.append(evidence)
                    paragraph_count += 1
                elif len(selected) < max_count:
                    selected.append(evidence)
        
        # æœ€çµ‚æ’åº
        selected.sort(key=lambda x: x.get('similarity', 0), reverse=True)
        
        page_distribution = {}
        for evidence in selected:
            page = evidence.get('page', evidence.get('index', 0))
            page_distribution[page] = page_distribution.get(page, 0) + 1
        
        logger.debug(f"å¤šæ¨£åŒ–é¸æ“‡: {len(selected)} è­‰æ“šåˆ†ä½ˆåœ¨ {len(page_distribution)} é : {page_distribution}")
        return selected
    
    def _fast_evidence_matching(self, text_chunks: List[str], 
                              article: Dict, 
                              max_evidences: int = 3) -> List[Dict]:
        """å¿«é€Ÿè­‰æ“šåŒ¹é…"""
        evidences = []
        
        # ç°¡åŒ–çš„é—œéµè©åŒ¹é…
        article_keywords = article.get('keywords', [])
        
        for chunk in text_chunks[:10]:  # é™åˆ¶æª¢æŸ¥æ•¸é‡
            score = 0.0
            matched_keywords = []
            
            # å¿«é€Ÿé—œéµè©è©•åˆ†
            chunk_lower = chunk.lower()
            for keyword in article_keywords[:5]:  # é™åˆ¶é—œéµè©æ•¸é‡
                if keyword.lower() in chunk_lower:
                    score += 0.2
                    matched_keywords.append(keyword)
            
            # å¤§å¹…é™ä½é–€æª»ï¼Œä¸¦æ·»åŠ èªç¾©åŒ¹é…å‚™æ¡ˆ
            if score > 0.05 or len(chunk.strip()) > 10:  # ä»»ä½•é—œéµè©åŒ¹é…æˆ–éç©ºå…§å®¹
                evidences.append({
                    'text': chunk[:200],  # æˆªæ–·é•·æ–‡æœ¬
                    'similarity': max(score, 0.1),  # ç¢ºä¿æœ‰åŸºæœ¬ç›¸ä¼¼åº¦åˆ†æ•¸
                    'score': score,
                    'matched_keywords': matched_keywords
                })
            
            if len(evidences) >= max_evidences:
                break
        
        return sorted(evidences, key=lambda x: x['score'], reverse=True)
    
    def _deduplicate_results(self, results: List[Dict]) -> List[Dict]:
        """å»é‡è¤‡çµæœ"""
        seen_articles = set()
        deduplicated = []
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        results.sort(key=lambda x: x['confidence'], reverse=True)
        
        for result in results:
            article_id = result['article']['article_id']
            if article_id not in seen_articles:
                seen_articles.add(article_id)
                deduplicated.append(result)
        
        return deduplicated[:50]  # é™åˆ¶çµæœæ•¸é‡

if __name__ == "__main__":
    # æ¸¬è©¦ä»£ç¢¼
    logger.info("ğŸ§ª FAISSå‘é‡æª¢ç´¢ç³»çµ±æ¸¬è©¦")
    
    # æ¨¡æ“¬IFRS S1æ¢æ–‡æ•¸æ“š
    mock_articles = [
        {
            'id': 'S1-20',
            'title': 'Purpose of IFRS S1',
            'content': 'This Standard requires an entity to disclose information about sustainability-related risks and opportunities.',
            'category': 'General Requirements',
            'difficulty': 'medium',
            'keywords': ['sustainability', 'risks', 'opportunities', 'disclosure']
        },
        {
            'id': 'S1-21',
            'title': 'Core content',
            'content': 'An entity shall disclose information that enables users to understand the governance processes.',
            'category': 'Governance',
            'difficulty': 'high',
            'keywords': ['governance', 'processes', 'users', 'understand']
        }
    ]
    
    # åˆå§‹åŒ–ä¸¦æ¸¬è©¦
    analyzer = FastIFRSAnalyzer()
    analyzer.initialize_with_articles(mock_articles)
    
    # æ¸¬è©¦æœç´¢
    test_results = analyzer.retrieval_system.hybrid_search(
        "sustainability governance disclosure", 
        top_k=5
    )
    
    print("ğŸ” æ¸¬è©¦æœç´¢çµæœ:")
    for result in test_results:
        print(f"  - {result['article_id']}: {result['title']} (åˆ†æ•¸: {result['score']:.3f})")
    
    # æ€§èƒ½çµ±è¨ˆ
    stats = analyzer.retrieval_system.get_stats()
    print(f"\nğŸ“Š æ€§èƒ½çµ±è¨ˆ: {stats}")